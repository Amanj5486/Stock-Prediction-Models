{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanj5486/Finance_ML/blob/main/Reinforcement_Learning_of_Stocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ie_7L5PKEVr"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import tensorflow as tf\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-J2sB2rMilC"
      },
      "source": [
        "is_ipython = 'inline'  in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2bRr2WIP11G"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=input_size,out_features=64)\n",
        "    self.fc2 = nn.Linear(in_features=64,out_features=32)\n",
        "    self.fc3 = nn.Linear(in_features=32,out_features=8)\n",
        "    self.out = nn.Linear(in_features=8,out_features=3)\n",
        "  def forward(self,t):\n",
        "    t = t.flatten(start_dim=1)\n",
        "    t = F.relu(self.fc1(t))\n",
        "    t = F.relu(self.fc2(t))\n",
        "    t = F.relu(self.fc3(t))\n",
        "    t = self.out(t)\n",
        "    return t  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phW3YcxCTDxE"
      },
      "source": [
        "Experience = namedtuple('Experience',('state','action','next_state','reward'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5z7XtmXT4i6"
      },
      "source": [
        "class ReplayMemory():\n",
        "  def __init__(self,capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "    self.push_count =0\n",
        "  def push(self,experience):\n",
        "    if len(self.memory)<self.capacity:\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      self.memory[self.push_count%self.capacity] = experience\n",
        "      self.push_count+=1\n",
        "  def sample(self,batch_size):\n",
        "    return random.sample(self.memory,batch_size)\n",
        "  def can_provide_sample(self,batch_size):\n",
        "   # print(len(self.memory))\n",
        "    return len(self.memory)>=batch_size        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExQsUJZpVDKR"
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "  def __init__(self,start,end,decay):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.decay = decay\n",
        "  def get_exploration_rate(self,current_step):\n",
        "    return self.end + (self.start-self.end)*math.exp(-1.*current_step*self.decay)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv5CYJcxWVtd"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self,strategy,num_actions,device):\n",
        "    self.current_step = 0 \n",
        "    self.strategy = strategy\n",
        "    self.num_actions = num_actions\n",
        "    self.device = device \n",
        "  def select_action(self,state,policy_net):\n",
        "    rate = strategy.get_exploration_rate(self.current_step)\n",
        "    self.current_step +=1\n",
        "    if rate> random.random():\n",
        "      action =  random.randrange(self.num_actions)  # explore\n",
        "      return torch.tensor([action]).to(device)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        #print(state,\"f1\")\n",
        "        return torch.tensor(policy_net(state).argmax(dim=1)).to(device) # exploit\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U_qxCeaJv6R"
      },
      "source": [
        "def formatPrice(n):\n",
        "    return(\"-Rs.\" if n<0 else \"Rs.\")+\"{0:.2f}\".format(abs(n))\n",
        "def getStockDataVec():\n",
        "    vec = []\n",
        "    lines = open(\"/content/GOOG.csv\",\"r\").read().splitlines()\n",
        "    for line in lines[1:2769]:\n",
        "        vec.append(float(line.split(\",\")[4]))\n",
        "    return vec \n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "def getState(data, t, n):\n",
        "    d = t - n + 1\n",
        "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
        "    res = []\n",
        "    for i in range(n - 1):\n",
        "        res.append(sigmoid(block[i + 1] - block[i]))\n",
        "    return np.array([res])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhTs6uokeOzd"
      },
      "source": [
        "def plot(values, moving_avg_period):\n",
        "    plt.figure(2)\n",
        "    plt.clf()        \n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(values)\n",
        "    plt.plot(get_moving_average(moving_avg_period, values))\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython: display.clear_output(wait=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FCkIN4XeUnw"
      },
      "source": [
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kalPQsCgfsN4"
      },
      "source": [
        "def extract_tensors(experiences):\n",
        "    # Convert batch of Experiences to Experience of batches\n",
        "    batch = Experience(*zip(*experiences))\n",
        "\n",
        "    t1 = torch.cat(batch.state)\n",
        "    t2 = torch.cat(batch.action)\n",
        "    t3 = torch.cat(batch.reward)\n",
        "    t4 = torch.cat(batch.next_state)\n",
        "\n",
        "    return (t1,t2,t3,t4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd5udAj2gNEC"
      },
      "source": [
        "class QValues():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    def get_current(policy_net, states, actions):\n",
        "      return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "    def get_next(target_net, next_states):\n",
        "      final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "      non_final_state_locations = (final_state_locations == False)\n",
        "      non_final_states = next_states[non_final_state_locations]\n",
        "      batch_size = next_states.shape[0]\n",
        "      values = torch.zeros(batch_size)#.to(QValues.device)\n",
        "      values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
        "      return values  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH4jZPpteVkA"
      },
      "source": [
        "batch_size = 256\n",
        "gamma = 0.9999\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.001\n",
        "target_update  = 10\n",
        "memory_size = 100000\n",
        "lr = 0.01\n",
        "num_episodes = 50\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpD5mN5Qbv2X"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#em = CartPoleEnvManager(device)\n",
        "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
        "agent = Agent(strategy, 3,device)\n",
        "memory = ReplayMemory(memory_size)\n",
        "vec = getStockDataVec()\n",
        "k=2267\n",
        "#print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3sOONybb_Vw"
      },
      "source": [
        "\n",
        "policy_net = DQN(64).to(device)\n",
        "target_net = DQN(64).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
        "window_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pO010MtqdE2A",
        "outputId": "05017722-30a5-48ab-f24a-93b77147a3b9"
      },
      "source": [
        "\n",
        "episode_durations = []\n",
        "for episode in range(num_episodes):\n",
        "    print(\"Episode \" + str(episode) + \"/\" + str(num_episodes))\n",
        "    state = torch.tensor(getState(vec, 0, window_size + 1))\n",
        "    total_profit = 0\n",
        "    invent = []\n",
        "    #state = getState(vec,)\n",
        "    max_transaction = 50 \n",
        "    total_money = 10000\n",
        "    c_s_h = 0\n",
        "    c_t_c =0\n",
        "    for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(1,action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "         #print(\"Buy: \" + formatPrice(data[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif action ==2 and len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "        reward = -vec[t]*b_p[0]+b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      if memory.can_provide_sample(batch_size):\n",
        "        experiences = memory.sample(batch_size)\n",
        "        states, actions, rewards, next_states = extract_tensors(experiences)\n",
        "        current_q_values = QValues.get_current(policy_net, states.float(), actions)\n",
        "        next_q_values = QValues.get_next(target_net, next_states.float())\n",
        "        target_q_values = (next_q_values * gamma) + rewards\n",
        "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        # episode_durations.append(t)\n",
        "        # plot(episode_durations, 100)\n",
        "         break\n",
        "    print(total_profit)    \n",
        "    if episode % target_update == 0:\n",
        "      target_net.load_state_dict(policy_net.state_dict())\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3909.9197654184177\n",
            "Episode 1/50\n",
            "22226.36941567988\n",
            "Episode 2/50\n",
            "1743.3526002522772\n",
            "Episode 3/50\n",
            "3165.3886265471306\n",
            "Episode 4/50\n",
            "2553.354734347198\n",
            "Episode 5/50\n",
            "279.8087408007957\n",
            "Episode 6/50\n",
            "250.2837543276607\n",
            "Episode 7/50\n",
            "347.732669708431\n",
            "Episode 8/50\n",
            "32.97980378172548\n",
            "Episode 9/50\n",
            "174.31105442896344\n",
            "Episode 10/50\n",
            "207.67136207441848\n",
            "Episode 11/50\n",
            "355.81776959694776\n",
            "Episode 12/50\n",
            "122.55784065034703\n",
            "Episode 13/50\n",
            "11.038777347460467\n",
            "Episode 14/50\n",
            "-13.972749441482478\n",
            "Episode 15/50\n",
            "68.41520544695476\n",
            "Episode 16/50\n",
            "143.87351059486423\n",
            "Episode 17/50\n",
            "-12.967523090273119\n",
            "Episode 18/50\n",
            "21.215939377419403\n",
            "Episode 19/50\n",
            "-7.363817630017024\n",
            "Episode 20/50\n",
            "19.130799774604213\n",
            "Episode 21/50\n",
            "149.3529138527621\n",
            "Episode 22/50\n",
            "91.54301138668092\n",
            "Episode 23/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-027c2bfbc48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_provide_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1371350de880>\u001b[0m in \u001b[0;36mextract_tensors\u001b[0;34m(experiences)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mt4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8C1uflrMtDu",
        "outputId": "cd2dc111-1b27-4508-9d8f-f8b4ca76f4f4"
      },
      "source": [
        "print(total_profit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "809.8879531428898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUUj96QSMyLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43b335d-88a7-4b70-e6d8-17f061fe01ca"
      },
      "source": [
        "vec = vec[2267:2769]\n",
        "print(len(vec))\n",
        "k=len(vec)\n",
        "total_profit = 0\n",
        "invent = []\n",
        "print(k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBf8e4cotg8V",
        "outputId": "ed648773-818c-42ca-a5bc-e084524c09d1"
      },
      "source": [
        "max_transaction = 50\n",
        "total_money = 10000\n",
        "c_s_h = 0\n",
        "c_t_c =0\n",
        "print(k)\n",
        "for t in range(k-1):\n",
        "      action = agent.select_action(state.float(), policy_net.float())\n",
        "      #print(action)\n",
        "      next_state =  torch.tensor(getState(vec, t+1, window_size + 1))\n",
        "      reward =0\n",
        "      if action == 0 and c_t_c < max_transaction and total_money>0:\n",
        "       # print(\"f2\")\n",
        "        x=total_money/(max_transaction-c_t_c) \n",
        "        total_money = total_money - x\n",
        "        c_t_c +=1\n",
        "        \n",
        "        x= x/vec[t]\n",
        "        c_s_h += x\n",
        "        a=[]\n",
        "        a.append(x)\n",
        "        a.append(vec[t])\n",
        "        invent.append(a)\n",
        "        print(\"Buy: \" + formatPrice(vec[t]))\n",
        "      elif action ==1 and len(invent)>0:\n",
        "        #print(\"f1\")\n",
        "        b_p = invent.pop(0)\n",
        "        reward = vec[t]*b_p[0]-b_p[0]*b_p[1]\n",
        "        total_money += vec[t]*b_p[0]\n",
        "        total_profit += reward \n",
        "        print('profit :', total_profit)\n",
        "        #print(reward)\n",
        "        c_s_h = c_s_h - b_p[0]\n",
        "        c_t_c =0\n",
        "      elif action ==2 and len(invent)>0:\n",
        "        b_p = invent[0]\n",
        "        #print(len(invent))\n",
        "       # print('f1')\n",
        "        reward = -vec[t]*b_p[0]+b_p[0]*b_p[1]  \n",
        "      #action = torch.tensor(action)  \n",
        "     \n",
        "      #print(action.shape)\n",
        "      reward = torch.tensor([reward])\n",
        "      memory.push(Experience(state, action, next_state, reward))\n",
        "      state = next_state\n",
        "      \n",
        "      done = True if t == k-2 else False  \n",
        "      if done:\n",
        "        #episode_durations.append(timestep)\n",
        "        #plot(episode_durations, 100)\n",
        "        break\n",
        "print(total_profit)\n",
        "print(total_money)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "Buy: Rs.1106.94\n",
            "profit : 4.524191073524548\n",
            "Buy: Rs.1089.52\n",
            "profit : 5.493869039792116\n",
            "Buy: Rs.1095.06\n",
            "profit : 8.158191588494788\n",
            "Buy: Rs.1021.57\n",
            "profit : 10.160672011338619\n",
            "Buy: Rs.1006.47\n",
            "profit : 13.87444680002568\n",
            "Buy: Rs.1019.97\n",
            "profit : 16.33676195977219\n",
            "Buy: Rs.1048.21\n",
            "profit : 17.594301567678315\n",
            "Buy: Rs.1082.76\n",
            "profit : 20.334704410597908\n",
            "Buy: Rs.1081.77\n",
            "profit : 19.745573544166632\n",
            "Buy: Rs.1079.24\n",
            "Buy: Rs.1075.66\n",
            "profit : 27.22111301131227\n",
            "profit : 38.626362143023556\n",
            "Buy: Rs.1169.84\n",
            "Buy: Rs.1103.98\n",
            "Buy: Rs.1115.65\n",
            "profit : 31.35293931963622\n",
            "profit : 40.238737113062484\n",
            "profit : 52.44542504918181\n",
            "Buy: Rs.1198.80\n",
            "Buy: Rs.1186.96\n",
            "profit : 63.3296875830423\n",
            "profit : 77.11224909901622\n",
            "Buy: Rs.1223.71\n",
            "profit : 80.16080561071638\n",
            "Buy: Rs.1061.49\n",
            "Buy: Rs.1044.41\n",
            "profit : 85.25269924087505\n",
            "profit : 94.90809948954526\n",
            "Buy: Rs.1106.43\n",
            "profit : 84.7605275281154\n",
            "Buy: Rs.1016.06\n",
            "profit : 95.60893787353032\n",
            "Buy: Rs.1095.01\n",
            "profit : 99.95140201541994\n",
            "Buy: Rs.1193.20\n",
            "profit : 99.97171619105279\n",
            "Buy: Rs.1231.54\n",
            "profit : 95.70058169181121\n",
            "Buy: Rs.1194.43\n",
            "profit : 97.85084588012512\n",
            "Buy: Rs.1272.18\n",
            "profit : 100.29556135806757\n",
            "Buy: Rs.1188.48\n",
            "profit : 95.89843898212555\n",
            "Buy: Rs.1042.22\n",
            "profit : 96.30916329765779\n",
            "Buy: Rs.1102.33\n",
            "profit : 93.38193052810192\n",
            "Buy: Rs.1079.80\n",
            "Buy: Rs.1076.01\n",
            "profit : 99.26147030833013\n",
            "profit : 106.82955387852886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Buy: Rs.1174.71\n",
            "Buy: Rs.1167.26\n",
            "profit : 109.37969657290733\n",
            "profit : 109.48012863415389\n",
            "Buy: Rs.1234.03\n",
            "Buy: Rs.1218.76\n",
            "profit : 111.52655072849532\n",
            "profit : 115.28081873829638\n",
            "Buy: Rs.1298.80\n",
            "profit : 115.15620032132017\n",
            "Buy: Rs.1320.70\n",
            "profit : 114.35354537614523\n",
            "Buy: Rs.1345.02\n",
            "profit : 115.14312999679979\n",
            "Buy: Rs.1348.84\n",
            "profit : 114.35123449600954\n",
            "114.35123449600954\n",
            "10114.351234496013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74vlgC6ft6Po"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}